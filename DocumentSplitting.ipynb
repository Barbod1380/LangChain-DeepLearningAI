{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Document Splitting**"
      ],
      "metadata": {
        "id": "OPzhY4xn20u8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ef_Lr79g23Vn",
        "outputId": "249e4488-b742-439e-d095-5afc664d0a31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.63-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.1 langchain-core-0.2.1 langchain-text-splitters-0.2.0 langsmith-0.1.63 orjson-3.10.3 packaging-23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use 2 different methods:\n",
        "- `RecursiveCharacterTextSplitter`\n",
        "- `CharacterTextSplitter`"
      ],
      "metadata": {
        "id": "TOUEJ5nS4bj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "\n",
        "chunk_size = 26\n",
        "chunk_overlap = 4"
      ],
      "metadata": {
        "id": "SM8ZwEVV3lXs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CharacterTextSplitter\n",
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size = chunk_size,\n",
        "    chunk_overlap = chunk_overlap\n",
        ")\n",
        "\n",
        "# RecursiveCharacterTextSplitter\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = chunk_size,\n",
        "    chunk_overlap = chunk_overlap\n",
        ")"
      ],
      "metadata": {
        "id": "gR9h6t_44lgC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's examine the splitters on our simple text:"
      ],
      "metadata": {
        "id": "NocBpPho4-kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = 'abcdefghijklmnopqrstuvwxyz'\n",
        "text2 = 'abcdefghijklmnopqrstuvwxyzabcdefg'\n",
        "\n",
        "print(len(text1))\n",
        "print(len(text2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slO44ae-5ElT",
        "outputId": "d70de7e9-7c8b-4b56-bbf6-d62054aacecf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n",
            "33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try CharacterTextSplitter on the first text:\n",
        "print(r_splitter.split_text(text1))\n",
        "\n",
        "# Try CharacterTextSplitter on the second text:\n",
        "print(r_splitter.split_text(text2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ffiCH_J5MCo",
        "outputId": "361288e6-f15b-4efd-86d6-cd5671fde3b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abcdefghijklmnopqrstuvwxyz']\n",
            "['abcdefghijklmnopqrstuvwxyz', 'wxyzabcdefg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine more complex text:"
      ],
      "metadata": {
        "id": "CIbBLHMt53in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text3 = \"a b c d e f g h i j k l m n o p q r s t u v w x y z\"\n",
        "\n",
        "print(f\"RecursiveCharacterTextSplitter: {r_splitter.split_text(text3)}\")\n",
        "print(f\"CharacterTextSplitter: {c_splitter.split_text(text3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfqE5d0J56cX",
        "outputId": "b948e6e9-9f7d-4725-b195-dec3812092ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RecursiveCharacterTextSplitter: ['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']\n",
            "CharacterTextSplitter: ['a b c d e f g h i j k l m n o p q r s t u v w x y z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `CharacterTextSplitter()` function, breaks the function based on the newlines first."
      ],
      "metadata": {
        "id": "UL555JGj6qsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size = chunk_size,\n",
        "    chunk_overlap = chunk_overlap,\n",
        "    separator = ' '\n",
        ")\n",
        "\n",
        "c_splitter.split_text(text3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3c5p1o663nu",
        "outputId": "2acff8f5-0474-489e-b5b7-a7d4085ac727"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a b c d e f g h i j k l m', 'l m n o p q r s t u v w x', 'w x y z']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Recursive splitting details**\n",
        "\n",
        "`RecursiveCharacterTextSplitter` is recommended for generic text."
      ],
      "metadata": {
        "id": "odyvpR2w7PqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
        "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
        "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
        "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
        "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
        "Sentences have a period at the end, but also, have a space.\\\n",
        "and words are separated by space.\"\"\"\n",
        "\n",
        "print(len(some_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mcVISwk7Tmn",
        "outputId": "a7d578b7-af8f-492c-e525-43c6c50ce2ca"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try this `some_text` using different methods:"
      ],
      "metadata": {
        "id": "sihpZXTa7Zf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## CharacterTextSplitter:\n",
        "c_splitter = CharacterTextSplitter(\n",
        "    chunk_size = 450,\n",
        "    chunk_overlap = 0,\n",
        "    separator = ' '\n",
        ")\n",
        "\n",
        "\n",
        "## RecursiveCharacterTextSplitter:\n",
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 450,\n",
        "    chunk_overlap = 0,\n",
        "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")"
      ],
      "metadata": {
        "id": "9Yt0f7L97eaM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = c_splitter.split_text(some_text)\n",
        "\n",
        "print(f\"Number Of Chunks: {len(chunks)}\")\n",
        "\n",
        "for chunk in chunks:\n",
        "    print(chunk)\n",
        "    print(\"-----------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QXTsp4V8Nwj",
        "outputId": "a6251153-cb2e-40d4-b5c4-72a41d3d060d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number Of Chunks: 2\n",
            "When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \n",
            "\n",
            " Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,\n",
            "-----------------\n",
            "have a space.and words are separated by space.\n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = r_splitter.split_text(some_text)\n",
        "\n",
        "print(f\"Number Of Chunks: {len(chunks)}\")\n",
        "\n",
        "for chunk in chunks:\n",
        "    print(chunk)\n",
        "    print(\"-----------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur0Lj2UD9ApY",
        "outputId": "a3c548e4-6a67-436e-a643-b364a692d2c4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number Of Chunks: 2\n",
            "When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\n",
            "-----------------\n",
            "Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.\n",
            "-----------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's reduce the chunk size a bit and add a period to our separators:"
      ],
      "metadata": {
        "id": "UY5kcoah9990"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 150,\n",
        "    chunk_overlap = 0,\n",
        "    separators = [\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "chunks = r_splitter.split_text(some_text)\n",
        "\n",
        "# Output the chunks:\n",
        "for chunk in chunks:\n",
        "    print(chunk)\n",
        "    print(\"-------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s_wLG999_gf",
        "outputId": "1c3ef4fb-c259-417b-8bd1-5608fef26ea9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example,\n",
            "-------------\n",
            "closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\n",
            "-------------\n",
            "Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this\n",
            "-------------\n",
            "string. Sentences have a period at the end, but also, have a space.and words are separated by space.\n",
            "-------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PDFs**\n",
        "\n",
        "So far, we have talked about how we can split a text into smaller pieces of chunks, next we will go for another type of data."
      ],
      "metadata": {
        "id": "qUMiW_xJ-vVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pypdf langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xNAen9nI--hz",
        "outputId": "4faeb0e3-1709-43d5-8832-5b94d90c8c31"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (4.2.0)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.6 langchain_community-0.2.1 marshmallow-3.21.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader('MachineLearning-Lecture01.pdf')\n",
        "pages = loader.load()"
      ],
      "metadata": {
        "id": "Veg0bHaG_2Va"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of pages: {len(pages)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge6VbZ4xALub",
        "outputId": "93f56826-3e11-4caf-b6fe-7fc45b47fcc5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages: 22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we create a simple splitter `CharacterTextSplitter`:"
      ],
      "metadata": {
        "id": "08mkdeDwAgtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap= 150,\n",
        "    length_function = len,\n",
        "    separator = '\\n'\n",
        ")"
      ],
      "metadata": {
        "id": "deve0K_8AniT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = text_splitter.split_documents(pages)\n",
        "\n",
        "print(f\"Number of chunks: {len(chunks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Fz15F_iBAbA",
        "outputId": "a695f364-09d9-46cd-ddc8-9fa8ca5c3b4f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We had $22$ pages but $77$ chunks."
      ],
      "metadata": {
        "id": "psvwyaZmBN6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Token splitting**\n",
        "\n",
        "- We can also split on token count explicity, if we want.\n",
        "\n",
        "- This can be useful because LLMs often have context windows designated in tokens.\n",
        "\n",
        "- Tokens are often ~4 characters."
      ],
      "metadata": {
        "id": "7y9vaVyuB2kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import TokenTextSplitter"
      ],
      "metadata": {
        "id": "4nD6EHWMB50L"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, Let's create our splitter. we also need to install `tiktoken`"
      ],
      "metadata": {
        "id": "SxR-L02OCGhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tiktoken"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4c0cFV1Cckk",
        "outputId": "955027db-0163-4975-ae3e-7b426314d17a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = TokenTextSplitter(\n",
        "    chunk_size = 1,\n",
        "    chunk_overlap = 0\n",
        ")\n",
        "\n",
        "text4 = \"foo bar bazzyfoo\""
      ],
      "metadata": {
        "id": "mn7mi85hCJ6h"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## split the text:\n",
        "text_splitter.split_text(text4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1g9772CCUZg",
        "outputId": "3f80e1a1-c007-4ace-c756-4dd994abd09a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['foo', ' bar', ' b', 'az', 'zy', 'foo']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that tokens are different from characters.\n",
        "now we apply this on the pdf document."
      ],
      "metadata": {
        "id": "-za2YSWlC5xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = TokenTextSplitter(\n",
        "    chunk_size = 10,\n",
        "    chunk_overlap = 0,\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "PK13YdF0DCzn"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    print(docs[i].page_content)\n",
        "    print(docs[i].metadata)\n",
        "    print(\"---------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZFUVvTuDLV_",
        "outputId": "4638ee99-e991-4ca6-fa42-260026b2e730"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MachineLearning-Lecture01  \n",
            "\n",
            "{'source': 'MachineLearning-Lecture01.pdf', 'page': 0}\n",
            "---------------\n",
            "Instructor (Andrew Ng):  Okay. Good\n",
            "{'source': 'MachineLearning-Lecture01.pdf', 'page': 0}\n",
            "---------------\n",
            " morning. Welcome to CS229, the machine \n",
            "{'source': 'MachineLearning-Lecture01.pdf', 'page': 0}\n",
            "---------------\n",
            "\n",
            "learning class. So what I wanna do today\n",
            "{'source': 'MachineLearning-Lecture01.pdf', 'page': 0}\n",
            "---------------\n",
            " is ju st spend a little time going over the\n",
            "{'source': 'MachineLearning-Lecture01.pdf', 'page': 0}\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Context aware splitting**\n",
        "\n",
        "Chunking aims to keep text with common context together.\n",
        "\n",
        "A text splitting often uses sentences or other delimiters to keep related text together but many documents (such as Markdown) have structure (headers) that can be explicitly used in splitting.\n",
        "\n",
        "We can use `MarkdownHeaderTextSplitter` to preserve header metadata in our chunks, as show below."
      ],
      "metadata": {
        "id": "kYdWAlFiDvdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "markdown_document = \"\"\"# Title\\n\\n \\\n",
        "## Chapter 1\\n\\n \\\n",
        "Hi this is Jim\\n\\n Hi this is Joe\\n\\n \\\n",
        "### Section \\n\\n \\\n",
        "Hi this is Lance \\n\\n\n",
        "## Chapter 2\\n\\n \\\n",
        "Hi this is Molly\"\"\""
      ],
      "metadata": {
        "id": "4h2mTSFaDxCZ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "]"
      ],
      "metadata": {
        "id": "whC5lD2vEIKw"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on = headers_to_split_on\n",
        ")\n",
        "\n",
        "md_header_splits = markdown_splitter.split_text(markdown_document)"
      ],
      "metadata": {
        "id": "NwSJ8eN1ERP0"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for split in md_header_splits:\n",
        "    print(split.page_content)\n",
        "    print(split.metadata)\n",
        "    print(\"----------------\")"
      ],
      "metadata": {
        "id": "6nVjkKJAFOul",
        "outputId": "f00a6129-3858-46fb-961d-864f613a3cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi this is Jim  \n",
            "Hi this is Joe\n",
            "{'Header 1': 'Title', 'Header 2': 'Chapter 1'}\n",
            "----------------\n",
            "Hi this is Lance\n",
            "{'Header 1': 'Title', 'Header 2': 'Chapter 1', 'Header 3': 'Section'}\n",
            "----------------\n",
            "Hi this is Molly\n",
            "{'Header 1': 'Title', 'Header 2': 'Chapter 2'}\n",
            "----------------\n"
          ]
        }
      ]
    }
  ]
}