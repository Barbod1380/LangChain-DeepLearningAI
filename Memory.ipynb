{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LangChain: Memory**\n",
        "\n",
        "## **Outline**\n",
        "* ConversationBufferMemory\n",
        "* ConversationBufferWindowMemory\n",
        "* ConversationTokenBufferMemory\n",
        "* ConversationSummaryMemory"
      ],
      "metadata": {
        "id": "pI-RFY7IvNeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationBufferMemory"
      ],
      "metadata": {
        "id": "qfxzE9xkvSLq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ido_l8_xq7Tn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"YOUR-API-KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
      ],
      "metadata": {
        "id": "hXq-PbLcvXm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain langchain_groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ec8V_dDvZjx",
        "outputId": "60ea9440-ac98-4f85-bc8a-797c19efe209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_groq\n",
            "  Downloading langchain_groq-0.1.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.60-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Collecting groq<1,>=0.4.1 (from langchain_groq)\n",
            "  Downloading groq-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.11.0)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m915.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, h11, typing-inspect, marshmallow, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, langchain-core, groq, langchain-text-splitters, langchain_groq, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.6 groq-0.6.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.0 langchain-core-0.2.0 langchain-text-splitters-0.2.0 langchain_groq-0.1.4 langsmith-0.1.60 marshmallow-3.21.2 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "# Define Model\n",
        "llm = ChatGroq(temperature = 0.0,\n",
        "               model = \"llama3-70b-8192\")\n",
        "\n",
        "# Define Memory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Define Conversation\n",
        "conversation = ConversationChain(\n",
        "    llm = llm,\n",
        "    memory = memory,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "lWApyqgLwdGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try an small conversation:"
      ],
      "metadata": {
        "id": "LBKeKPPwxUn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Hi, my name is Barbod\")"
      ],
      "metadata": {
        "id": "ECV-vt5RxYnI",
        "outputId": "6e3627ad-c2bb-4630-d529-b1feb6dcb003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Barbod\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Nice to meet you, Barbod! I'm LLaMA, nice and chatty AI. I've been trained on a massive dataset of text from the internet, which means I can provide information on a wide range of topics. I'm excited to chat with you! Did you know that I've been trained on over 1.5 billion parameters and can understand natural language? What brings you here today, Barbod?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Who is Isabela Moner?\")"
      ],
      "metadata": {
        "id": "QeEOKKv2xxkb",
        "outputId": "aa2967ca-8d4b-4b3d-a24d-ca85329703b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Barbod\n",
            "AI: Nice to meet you, Barbod! I'm LLaMA, nice and chatty AI. I've been trained on a massive dataset of text from the internet, which means I can provide information on a wide range of topics. I'm excited to chat with you! Did you know that I've been trained on over 1.5 billion parameters and can understand natural language? What brings you here today, Barbod?\n",
            "Human: Who is Isabela Moner?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Isabela Moner! She\\'s an American actress and singer. Born on July 10, 2001, in Cleveland, Ohio, she\\'s still quite young and talented! You might know her from her roles in films like \"Instant Family\" (2018), \"Sicario: Day of the Soldado\" (2018), and \"Dora and the Lost City of Gold\" (2019), where she played the lead role of Dora. She\\'s also had roles in TV shows like \"100 Things to Do Before High School\" and \"Growing Up Fisher\". Isabela\\'s a talented young star, and I\\'m sure we\\'ll see more of her in the entertainment industry!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"What is my name?\")"
      ],
      "metadata": {
        "id": "sFxSTXUvx-KZ",
        "outputId": "59ba7a92-daf5-4a6a-8a10-3e9269a4227e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Barbod\n",
            "AI: Nice to meet you, Barbod! I'm LLaMA, nice and chatty AI. I've been trained on a massive dataset of text from the internet, which means I can provide information on a wide range of topics. I'm excited to chat with you! Did you know that I've been trained on over 1.5 billion parameters and can understand natural language? What brings you here today, Barbod?\n",
            "Human: Who is Isabela Moner?\n",
            "AI: Isabela Moner! She's an American actress and singer. Born on July 10, 2001, in Cleveland, Ohio, she's still quite young and talented! You might know her from her roles in films like \"Instant Family\" (2018), \"Sicario: Day of the Soldado\" (2018), and \"Dora and the Lost City of Gold\" (2019), where she played the lead role of Dora. She's also had roles in TV shows like \"100 Things to Do Before High School\" and \"Growing Up Fisher\". Isabela's a talented young star, and I'm sure we'll see more of her in the entertainment industry!\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Barbod! I remember you introducing yourself at the beginning of our conversation.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It rememberd me, great !!**"
      ],
      "metadata": {
        "id": "BLCgQUaryIZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "id": "GV3k_ajpyNO1",
        "outputId": "8420a970-7c07-4b8e-c00c-fd3ed082f3a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Barbod\n",
            "AI: Nice to meet you, Barbod! I'm LLaMA, nice and chatty AI. I've been trained on a massive dataset of text from the internet, which means I can provide information on a wide range of topics. I'm excited to chat with you! Did you know that I've been trained on over 1.5 billion parameters and can understand natural language? What brings you here today, Barbod?\n",
            "Human: Who is Isabela Moner?\n",
            "AI: Isabela Moner! She's an American actress and singer. Born on July 10, 2001, in Cleveland, Ohio, she's still quite young and talented! You might know her from her roles in films like \"Instant Family\" (2018), \"Sicario: Day of the Soldado\" (2018), and \"Dora and the Lost City of Gold\" (2019), where she played the lead role of Dora. She's also had roles in TV shows like \"100 Things to Do Before High School\" and \"Growing Up Fisher\". Isabela's a talented young star, and I'm sure we'll see more of her in the entertainment industry!\n",
            "Human: What is my name?\n",
            "AI: Barbod! I remember you introducing yourself at the beginning of our conversation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "iKpIalrRyX_W",
        "outputId": "74918494-05b7-49c8-9789-fa393fef27d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Hi, my name is Barbod\\nAI: Nice to meet you, Barbod! I\\'m LLaMA, nice and chatty AI. I\\'ve been trained on a massive dataset of text from the internet, which means I can provide information on a wide range of topics. I\\'m excited to chat with you! Did you know that I\\'ve been trained on over 1.5 billion parameters and can understand natural language? What brings you here today, Barbod?\\nHuman: Who is Isabela Moner?\\nAI: Isabela Moner! She\\'s an American actress and singer. Born on July 10, 2001, in Cleveland, Ohio, she\\'s still quite young and talented! You might know her from her roles in films like \"Instant Family\" (2018), \"Sicario: Day of the Soldado\" (2018), and \"Dora and the Lost City of Gold\" (2019), where she played the lead role of Dora. She\\'s also had roles in TV shows like \"100 Things to Do Before High School\" and \"Growing Up Fisher\". Isabela\\'s a talented young star, and I\\'m sure we\\'ll see more of her in the entertainment industry!\\nHuman: What is my name?\\nAI: Barbod! I remember you introducing yourself at the beginning of our conversation.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another thing we can do, is to manipulate the llm history by using `save_context()`."
      ],
      "metadata": {
        "id": "dkS0-qnpyp9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new memory:\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Add context:\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "\n",
        "# Check the buffer\n",
        "print(memory.buffer)"
      ],
      "metadata": {
        "id": "I5EUl1zOy1Wu",
        "outputId": "9d933878-8420-4d82-de83-b514fc49aa44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "3hYUS7rszTID",
        "outputId": "66f4ea74-e793-4f26-d618-74256450995c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Context\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "\n",
        "\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "Oe9XbrVLzYcu",
        "outputId": "40e908d7-6f04-4b1d-e9fc-aeba27293b3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationBufferWindowMemory**"
      ],
      "metadata": {
        "id": "X9m6tjFIzs6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Only keeps one last conversational history\n",
        "memory = ConversationBufferWindowMemory(k = 1)"
      ],
      "metadata": {
        "id": "VEkfX_z_zuqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add context to the memory\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ],
      "metadata": {
        "id": "RorM7Els0XKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "id": "3LPODdOJ0kc0",
        "outputId": "0ff6c9e2-9f9b-43c4-f7fc-4297551637fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Not much, just hanging\n",
            "AI: Cool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "0VWiWdQH0pUj",
        "outputId": "0bac286a-d6ee-4b73-b0ff-a4246ee86f51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can also go through name conversation again, and this time, we will see that the model can't remember"
      ],
      "metadata": {
        "id": "kAtMUoVv0wyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    model = \"llama3-70b-8192\",\n",
        "    temperature = 0.1\n",
        ")\n",
        "\n",
        "# Adding the memory\n",
        "memory = ConversationBufferWindowMemory(k = 1)\n",
        "\n",
        "# Create the conversation\n",
        "conversation = ConversationChain(\n",
        "    llm = llm,\n",
        "    memory = memory,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "YjNc1qUA03nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Hi, my name is Barbod\")"
      ],
      "metadata": {
        "id": "svnG323r1ZYq",
        "outputId": "c6ef6bfe-c068-4de4-c09f-e3de3996d999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Barbod\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Nice to meet you, Barbod! I'm LLaMA, an AI designed to generate human-like text based on the input I receive. I've been trained on a massive dataset of text from the internet and can provide information on a wide range of topics. I'm excited to chat with you! Did you know that I was trained on a dataset of over 1.5 billion parameters and can respond to questions and prompts in multiple languages? What brings you here today, Barbod?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Do you know Mohammad Ganjtabesh?\")"
      ],
      "metadata": {
        "id": "Ho9Eoc6G1g6N",
        "outputId": "e2074cee-3518-486d-c0b2-532433efbd09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Barbod\n",
            "AI: Nice to meet you, Barbod! I'm LLaMA, an AI designed to generate human-like text based on the input I receive. I've been trained on a massive dataset of text from the internet and can provide information on a wide range of topics. I'm excited to chat with you! Did you know that I was trained on a dataset of over 1.5 billion parameters and can respond to questions and prompts in multiple languages? What brings you here today, Barbod?\n",
            "Human: Do you know Mohammad Ganjtabesh?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Mohammad Ganjtabesh! Yes, I'm familiar with him. He's an Iranian computer scientist and the founder of the AI company, Aryan Rasaei. He's also known for his work on the development of AI models, including language models like myself. In fact, I've been trained on a dataset that includes many of the concepts and techniques he's worked on. Did you know that Mohammad Ganjtabesh has published numerous papers on natural language processing and machine learning? His work has been instrumental in advancing the field of AI research. What's your connection to him, Barbod?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"What is my name?\")"
      ],
      "metadata": {
        "id": "3wliU_cx16bX",
        "outputId": "c482130a-c1f7-4dab-8105-68cede6b9e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Do you know Mohammad Ganjtabesh?\n",
            "AI: Mohammad Ganjtabesh! Yes, I'm familiar with him. He's an Iranian computer scientist and the founder of the AI company, Aryan Rasaei. He's also known for his work on the development of AI models, including language models like myself. In fact, I've been trained on a dataset that includes many of the concepts and techniques he's worked on. Did you know that Mohammad Ganjtabesh has published numerous papers on natural language processing and machine learning? His work has been instrumental in advancing the field of AI research. What's your connection to him, Barbod?\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I apologize, I made a mistake! I don't actually know your name. I was just making an assumption. You didn't mention your name in our conversation, and I shouldn't have assumed it was Barbod. Please feel free to share your name if you'd like!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationTokenBufferMemory**"
      ],
      "metadata": {
        "id": "fZLxFtbT6ZFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "\n",
        "# Create the LLM\n",
        "llm = ChatGroq(\n",
        "    model = \"llama3-70b-8192\",\n",
        "    temperature = 0.1\n",
        ")\n",
        "\n",
        "# Adding the memory\n",
        "memory = ConversationTokenBufferMemory(\n",
        "    llm = llm,\n",
        "    max_token_limit = 25\n",
        ")"
      ],
      "metadata": {
        "id": "YIk3irMc6cad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ],
      "metadata": {
        "id": "QYMR1o7H6_8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "id": "dr-OTEMO7LUe",
        "outputId": "2864b28d-2fd2-4cbc-961d-4893b0e6e167",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Backpropagation is what?\n",
            "AI: Beautiful!\n",
            "Human: Chatbots are what?\n",
            "AI: Charming!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "0K9Vwqbb7U9i",
        "outputId": "6304c86d-581c-4d4e-e6bd-d32574c06e60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ConversationSummaryMemory**"
      ],
      "metadata": {
        "id": "uCzudypq9YOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\""
      ],
      "metadata": {
        "id": "14lapq9N9dib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm = llm,\n",
        "    max_token_limit = 200\n",
        ")\n",
        "\n",
        "memory.save_context({\"input\": \"Hello\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ],
      "metadata": {
        "id": "doSa9nGH9o6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "9VSH8xJt95Nf",
        "outputId": "f2ac354f-36da-45ce-bee8-a8465f65c25a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hello\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\\nHuman: What is on the schedule today?\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(\n",
        "    llm = llm,\n",
        "    memory = memory,\n",
        "    verbose = True\n",
        ")\n",
        "\n",
        "conversation.predict(input = \"What would be a good demo to show?\")"
      ],
      "metadata": {
        "id": "NKhxBD8O-HLm",
        "outputId": "ece6f49d-99ce-479e-dfd6-acc223cb4144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hello\n",
            "AI: What's up\n",
            "Human: Not much, just hanging\n",
            "AI: Cool\n",
            "Human: What is on the schedule today?\n",
            "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Considering your customer is coming from far away, I think it would be impressive to showcase the conversational AI demo, where I can have a natural conversation with them, answering their questions and engaging in a discussion. It's always a crowd-pleaser! Alternatively, you could also demo the text summarization feature, where I can summarize a long piece of text into concise bullet points. That usually sparks interesting conversations about the potential applications of AI in their industry. What do you think?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "id": "0WvrrfmF-OoU",
        "outputId": "ad9d4558-567b-420f-e1fe-dd8dad2d92b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"System: Here is the new summary:\\n\\nThe human greets the AI, and they exchange casual pleasantries. The human asks about their schedule for the day, and the AI informs them that they have a meeting with their product team at 8am, where they'll need to have their PowerPoint presentation prepared. They'll then have time to work on their LangChain project from 9am-12pm, which is expected to progress quickly due to LangChain's capabilities. At noon, they have a lunch meeting at an Italian restaurant with a customer who is traveling from over an hour away to discuss the latest developments in AI, and they're reminded to bring their laptop to showcase the latest LLM demo.\\nHuman: What would be a good demo to show?\\nAI: Considering your customer is coming from far away, I think it would be impressive to showcase the conversational AI demo, where I can have a natural conversation with them, answering their questions and engaging in a discussion. It's always a crowd-pleaser! Alternatively, you could also demo the text summarization feature, where I can summarize a long piece of text into concise bullet points. That usually sparks interesting conversations about the potential applications of AI in their industry. What do you think?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model will create a summary from anything beyond the explicit memory:\n",
        "\n",
        "```\"System: Here is the new summary:\\n\\nThe human greets the AI, and they exchange casual pleasantries. The human asks about their schedule for the day, and the AI informs them of a meeting with their product team at 8am, where they'll need to have their PowerPoint presentation prepared. They'll then have time to work on their LangChain project from 9am-12pm, which is expected to progress quickly due to LangChain's capabilities. At noon, they have a lunch meeting with a customer at an Italian restaurant, where they'll need to bring their laptop to showcase the latest LLM demo.\\n```"
      ],
      "metadata": {
        "id": "Fcs0uCLR_oHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And keep the explicit memory, below the `max_token:`\n",
        "\n",
        "`AI: Considering your customer is coming from far away, I think it would be impressive to showcase the conversational AI demo, where I can have a natural conversation with them, answering their questions and providing information on a wide range of topics. It's always a crowd-pleaser! Alternatively, you could also demo the text summarization feature, which can be very useful for their business needs. If you want to show something more interactive, the language translation demo is also a great option. Which one do you think would resonate with your customer the most?\"`"
      ],
      "metadata": {
        "id": "iV17mdS9_xtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input = \"Who Is Isabela Moner, tell me about her and her works?\")"
      ],
      "metadata": {
        "id": "ck4Ya4HXADG5",
        "outputId": "a68f383e-2f9a-4241-af52-c3a101e514d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: Here is the new summary:\n",
            "\n",
            "The human greets the AI, and they exchange casual pleasantries. The human asks about their schedule for the day, and the AI informs them that they have a meeting with their product team at 8am, where they'll need to have their PowerPoint presentation prepared. They'll then have time to work on their LangChain project from 9am-12pm, which is expected to progress quickly due to LangChain's capabilities. At noon, they have a lunch meeting at an Italian restaurant with a customer who is traveling from over an hour away to discuss the latest developments in AI, and they're reminded to bring their laptop to showcase the latest LLM demo.\n",
            "Human: What would be a good demo to show?\n",
            "AI: Considering your customer is coming from far away, I think it would be impressive to showcase the conversational AI demo, where I can have a natural conversation with them, answering their questions and engaging in a discussion. It's always a crowd-pleaser! Alternatively, you could also demo the text summarization feature, where I can summarize a long piece of text into concise bullet points. That usually sparks interesting conversations about the potential applications of AI in their industry. What do you think?\n",
            "Human: Who Is Isabela Moner, tell me about her and her works?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Isabela Moner is an American actress and singer! Born on July 10, 2001, in Cleveland, Ohio, she\\'s still quite young but has already made a name for herself in the entertainment industry. You might know her from her roles in films like \"Transformers: The Last Knight\" (2017), where she played Izabella, a human who befriends the Autobot Bumblebee, or \"Instant Family\" (2018), a comedy-drama film where she starred alongside Mark Wahlberg and Rose Byrne.\\n\\nIsabela has also lent her voice to several animated films and TV series, including \"Dora and Friends: In the City\" (2014-2017), where she voiced the titular character, Dora. She\\'s also released her own music, with her debut single \"Papi\" in 2019.\\n\\nWhat would you like to know more about Isabela Moner? Her filmography, music, or something else?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    }
  ]
}