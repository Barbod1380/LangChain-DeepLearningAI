{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Chains in LangChain**\n",
        "\n",
        "## **Outline**\n",
        "\n",
        "* LLMChain\n",
        "* Sequential Chains\n",
        "  * SimpleSequentialChain\n",
        "  * SequentialChain\n",
        "* Router Chain"
      ],
      "metadata": {
        "id": "rHLdRgNFOKw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers."
      ],
      "metadata": {
        "id": "YNJ6bIgNOc2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install langchain langchain_groq"
      ],
      "metadata": {
        "id": "13xiP7ZXOeSh",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LLMChain**\n",
        "\n",
        "For using `ChatGroq`, we first need to set API Key."
      ],
      "metadata": {
        "id": "hjCKvwdiOmnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"YOUR-API-KEY\""
      ],
      "metadata": {
        "id": "_0taVDcOPJnt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LLMChain` is a class that facilitates the creation of chains that combine language models with other functionalities.\n",
        "\n",
        "It simplifies the process of setting up a workflow where an LLM is used to process input and generate output, possibly in conjunction with other steps."
      ],
      "metadata": {
        "id": "B-gRqPRi2Yyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "\n",
        "## Create the langchain model:\n",
        "llm = ChatGroq(\n",
        "    model = \"llama3-70b-8192\",\n",
        "    temperature = 0.9\n",
        ")"
      ],
      "metadata": {
        "id": "X0S0sLkgOpI6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Components of LLMChain**\n",
        "\n",
        "- **LLM:**\n",
        "This is the language model you are using. It can be any model that conforms to the LangChainâ€™s interface for language models.\n",
        "\n",
        "- **Prompt:** This defines how the input should be formatted and what specific instructions or context should be given to the language model.\n",
        "\n",
        "- **Output Parsing:** This handles the parsing of the output from the language model into a format that can be used for subsequent steps or final output."
      ],
      "metadata": {
        "id": "rytg-9292uc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "# Create the chain:\n",
        "chain = LLMChain(llm = llm,\n",
        "                 prompt = prompt)"
      ],
      "metadata": {
        "id": "PGXvKDSjPcH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5e1702-ce90-47f8-9b45-846c6ca3cdca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "product = \"Hats with shape of animation characters.\"\n",
        "\n",
        "# Invoke the chain\n",
        "display(Markdown(chain.run(product)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "mkktbdR8PxKC",
        "outputId": "78e549f9-8f3f-486f-968c-3075ab12ee68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "What a fun question!\n\nHere are some name suggestions for a company that makes hats with shapes of animation characters:\n\n1. **ToonTopia**: A playful combination of \"toon\" (short for cartoon) and \"utopia,\" implying a fantastical world of animated characters on hats.\n2. **Character Caps**: Simple and straightforward, emphasizing the focus on character-shaped hats.\n3. **Hatimation**: A blend of \"hat\" and \"animation,\" highlighting the unique fusion of fashion and animation.\n4. **Pixie Peaks**: \"Pixie\" evokes a sense of playful, whimsical designs, while \"Peaks\" references the hat shapes.\n5. **Fable Factory**: \"Fable\" nods to the fantastical stories and characters, while \"Factory\" implies a creative, industrious approach to hat-making.\n6. **Cartoon Couture**: This name combines the world of cartoons with high-fashion \"couture,\" suggesting stylish, character-inspired hats.\n7. **Animate Apparel**: This name emphasizes the animated characters while also highlighting the clothing aspect of the hats.\n8. **Storybook Style**: This name taps into the nostalgic, storytelling aspect of animation, implying hats that evoke a sense of wonder and imagination.\n9. **Charm Caps**: \"Charm\" conveys the delightful, whimsical nature of the hats, while \"Caps\" is a colloquialism for hats.\n10. **Fantasy Fusion**: This name highlights the blending of fantasy worlds (animation) with functional fashion (hats).\n\nPick the one that resonates with your brand's personality and style!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SimpleSequentialChain**\n",
        "\n",
        "- The simplest form of a sequential chain is where each step has a single input and output.\n",
        "\n",
        "- The output of one step is passed as input to the next step in the chain. You would use `SimpleSequentialChain` when you have a **linear pipeline** where each step has a single input and output, it implicitly passes the output of one step as input to the next.\n",
        "\n",
        "- This is great for composing a precise sequence of LLMChains where each builds directly on the previous output."
      ],
      "metadata": {
        "id": "ReFGCmz_RELP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "## Create the llm model:\n",
        "llm = ChatGroq(\n",
        "    model = \"llama3-70b-8192\",\n",
        "    temperature = 0.8\n",
        ")\n",
        "\n",
        "## Create the prompt template:\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}? Give just one name.\"\n",
        ")\n",
        "\n",
        "## Create the chain:\n",
        "chain_one = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = first_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "GwYDen0HRIMZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will create the second chain:"
      ],
      "metadata": {
        "id": "zWQYtt12370O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Template for the second prompt\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following, make sure to use the company name in this description. \\\n",
        "    company:{company_name}\"\n",
        ")\n",
        "\n",
        "## Create the second chain:\n",
        "chain_two = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = second_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "4TjAhBOG3-RI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next step, we use the `SimpleSequentialChain` to connect those two chains:\n",
        "\n",
        "### **What is SimpleSequentialChain?**\n",
        "`SimpleSequentialChain` helps in chaining together multiple `LLMChain` instances or other chains in a linear sequence.\n",
        "\n",
        "This allows you to build complex workflows where the data flows through each step in a defined order."
      ],
      "metadata": {
        "id": "fkJXoOJk4Rv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain = SimpleSequentialChain(\n",
        "    chains = [chain_one, chain_two],\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "ZxFWIcrN4Y40"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Product is: \\\"{product}\\\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCpLElZd5Fob",
        "outputId": "f8e349e5-3dab-4691-9cda-95cce4e304fb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product is: \"Hats with shape of animation characters.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "aspQbwMFRp7-",
        "outputId": "bce0e6bc-3dd6-4415-a5fa-e73530b03d81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mWhat a unique and fun idea!\n",
            "\n",
            "Here's a suggestion for a company name that might fit the bill:\n",
            "\n",
            "**ToonTops**\n",
            "\n",
            "This name plays off the idea of \"toons\" (short for cartoons or animations) and \"tops\" (referring to hats being worn on top of the head). It's catchy, easy to remember, and immediately conveys the theme of the company's products.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mToonTops brings whimsy to wearables with customizable, interchangeable hat toppers featuring beloved cartoon characters and quirky designs.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ToonTops brings whimsy to wearables with customizable, interchangeable hat toppers featuring beloved cartoon characters and quirky designs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SequentialChain**\n",
        "\n",
        "- A more general form of sequential chain allows multiple inputs and outputs per step.\n",
        "\n",
        "- You would use `SequentialChain` when you have a more complex pipeline where steps might have multiple inputs and outputs.\n",
        "\n",
        "- `SequentialChain` allows you to explicitly specify all the input and output variables at each step and map outputs from one step to inputs of the next. This provides more flexibility when steps might have multiple dependencies or produce multiple results to pass along."
      ],
      "metadata": {
        "id": "HCxcNmdlTFFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model = \"llama3-70b-8192\",\n",
        "    temperature = 0.9\n",
        ")\n",
        "\n",
        "## prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to english:\"\n",
        "    \"\\n\\n{Review}\"\n",
        ")\n",
        "\n",
        "## chain1: Review ---> Translate into English\n",
        "chain_one = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = first_prompt,\n",
        "    output_key = \"English_Review\"\n",
        ")"
      ],
      "metadata": {
        "id": "o5PfC9n9TIsK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## prompt template 2: summarize the english text\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\"\n",
        "    \"\\n\\n{English_Review}\"\n",
        ")\n",
        "\n",
        "# chain2: English Review ---> Summarize\n",
        "chain_two = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = second_prompt,\n",
        "    output_key = \"summary\"\n",
        ")"
      ],
      "metadata": {
        "id": "BZr6dy7QTmlX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 3: Find the language of the initial review\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "\n",
        "# chain3: Review ---> language\n",
        "chain_three = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = third_prompt,\n",
        "    output_key = \"language\"\n",
        ")"
      ],
      "metadata": {
        "id": "Y3AAPhx9UOps"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary just in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "\n",
        "# chain4: summary, language ---> Response\n",
        "chain_four = LLMChain(llm = llm,\n",
        "                      prompt = fourth_prompt,\n",
        "                      output_key = \"followup_message\"\n",
        "                     )"
      ],
      "metadata": {
        "id": "Xmp0nkpOUfY2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_chain: Review ---> English_Review, summary, language, followup_message\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains = [chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables = [\"Review\"],\n",
        "    output_variables = [\"English_Review\", \"summary\", \"language\", \"followup_message\"],\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "rE-9_S-MU6Er"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"Creo que es un producto maravilloso, su precio es un poco elevado, pero creo que merece la pena.\"\n",
        "\n",
        "overall_chain(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ly8EbhPUWRpm",
        "outputId": "3dd30ae2-bb02-4ad2-d196-4dd69c77d7f8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': 'Creo que es un producto maravilloso, su precio es un poco elevado, pero creo que merece la pena.',\n",
              " 'English_Review': 'Here is the translation:\\n\\n\"I think it\\'s a wonderful product, the price is a bit high, but I think it\\'s worth it.\"',\n",
              " 'summary': 'Here is a summary of the review in 1 sentence:\\n\\nThe reviewer thinks the product is wonderful and worth the high price.',\n",
              " 'language': 'The language of the review is Spanish.\\n\\nHere\\'s a translation of the review:\\n\\n\"I think it\\'s a wonderful product, its price is a bit high, but I think it\\'s worth it.\"',\n",
              " 'followup_message': 'Excelente elecciÃ³n! El producto es verdaderamente increÃ­ble y la inversiÃ³n vale la pena.'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **When to use `SequentialChain`**\n",
        "\n",
        "- When your workflow requires more complex logic, such as conditionally executing steps based on intermediate results.\n",
        "- When you need to manipulate the inputs and outputs between steps in more detailed ways.\n",
        "- For workflows that involve branching, looping, or more advanced error handling."
      ],
      "metadata": {
        "id": "93UayIxF8mcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Router Chain**\n",
        "\n",
        "- Router chains allow routing inputs to different destination chains based on the input text. This allows the building of chatbots and assistants that can handle diverse requests.\n",
        "\n",
        "  - Router chains examine the input text and route it to the appropriate\n",
        "  destination chain.\n",
        "  - Destination chains handle the actual execution based on the input.\n",
        "  - Router chains are powerful for building multi-purpose chatbots/assistants.\n"
      ],
      "metadata": {
        "id": "s-ep8Hx1hYC6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Physics & Math Template:\n",
        "\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "ypHM45F9UAdA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## History & ComputerScience Template:\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "FW5ixEYZhcgC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we merge these four templates into a list of dictionaries that each dictionary has three components:\n",
        "- `name`\n",
        "- `description`\n",
        "- `prompt_template`\n",
        "\n",
        "We will use the `name` and `prompt_template` to create"
      ],
      "metadata": {
        "id": "fSp3QoEvUUrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "WakfOJsjUdCD"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have defined different routers prompt templates, we create the llm"
      ],
      "metadata": {
        "id": "nqk9RVy5U7lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MultiPromptChain:**\n",
        "\n",
        "The `MultiPromptChain` works by defining multiple prompts and a mechanism to decide which prompt to use for a given input. It uses a router to make this decision, and each prompt can be tailored to handle a specific type of query or task.\n",
        "It takes multiple arguments as its inputs:\n",
        "\n",
        "- **`router_chain`**: The `router_chain` is a specialized chain responsible for determining which sub-chain or prompt should handle the input. It is essentially the decision-making component.\n",
        "\n",
        "- **`destination_chains`**: These are the actual sub-chains or pipelines that handle specific types of inputs. Each destination_chain corresponds to a particular task or type of query.\n",
        "\n",
        "- **`default_chain`**: The `default_chain` is a fallback or catch-all chain that handles any inputs that do not match the criteria for the destination_chains."
      ],
      "metadata": {
        "id": "8l9SuR7Fbfnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "## LLM\n",
        "llm = ChatGroq(\n",
        "    model = \"llama3-70b-8192\",\n",
        "    temperature = 0.1\n",
        ")"
      ],
      "metadata": {
        "id": "ygL-5nhSc8vN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So first, we want to create the `router_chain`, for that, we use the **`LLMRouterChain`**."
      ],
      "metadata": {
        "id": "oJrPyBY0c_m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## First, we need to store all different routs with their names and chains into a dictionary.\n",
        "destination_chains = {}\n",
        "\n",
        "for p_info in prompt_infos:\n",
        "\n",
        "    ## Extract the name and template\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "\n",
        "    ## Convert the raw text template into a prompt template\n",
        "    prompt = ChatPromptTemplate.from_template(template = prompt_template)\n",
        "\n",
        "    ## Create the chain based on the prompt\n",
        "    chain = LLMChain(llm = llm,\n",
        "                     prompt = prompt)\n",
        "\n",
        "    ## Add the chain with its corresponding name to dictionary\n",
        "    destination_chains[name] = chain"
      ],
      "metadata": {
        "id": "ATE7IFjbiRAp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\""
      ],
      "metadata": {
        "id": "cF_a9UezkF2w"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to feel the `destination` in our prompt, in consist of a name and the description corresponds to that name."
      ],
      "metadata": {
        "id": "TxEUsZAle0I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "print(destinations_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r02EVRkjecKQ",
        "outputId": "768e56a7-362b-40a5-d596-d35967a754fb"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "physics: Good for answering questions about physics\n",
            "math: Good for answering math questions\n",
            "History: Good for answering history questions\n",
            "computer science: Good for answering computer science questions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Fill the destination\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations = destinations_str\n",
        ")\n",
        "\n",
        "\n",
        "## Create the prompt\n",
        "router_prompt = PromptTemplate(\n",
        "    template = router_template,\n",
        "    input_variables = [\"input\"],\n",
        "    output_parser = RouterOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "## Create the chain\n",
        "router_chain = LLMRouterChain.from_llm(\n",
        "    llm = llm,\n",
        "    prompt = router_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "tRbIjkP1kKLB"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Checkout the final router prompt:\n",
        "router_template"
      ],
      "metadata": {
        "id": "kMeu0dr9mfyt",
        "outputId": "ee55a02c-9abe-46fa-d13a-6c75abadd8a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revisingit will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{\\n    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\ a potentially modified version of the original input\\n}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\nphysics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions\\n\\n<< INPUT >>\\n{input}\\n\\n<< OUTPUT (remember to include the ```json)>>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next tep is to create the default chain, A fallback chain for inputs that do not match any specific criteria."
      ],
      "metadata": {
        "id": "miu8-7ODgLn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"Say 'HELLO LEONARDO' at the begging and then answer the question that has been mentioned in the following 'input': {input}\")\n",
        "\n",
        "default_chain = LLMChain(\n",
        "    llm = llm,\n",
        "    prompt = default_prompt\n",
        ")"
      ],
      "metadata": {
        "id": "3_yfAxqogSwI"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINALLY**"
      ],
      "metadata": {
        "id": "ZbQj_BC7ge43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Connect the router, destination and default chain to eachother:\n",
        "chain = MultiPromptChain(\n",
        "    router_chain = router_chain,\n",
        "    destination_chains = destination_chains,\n",
        "    default_chain = default_chain,\n",
        "    verbose = True\n",
        ")"
      ],
      "metadata": {
        "id": "M0O3o8KSk7Ja"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"What is black body radiation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "139oY4pVlCwo",
        "outputId": "e62b0e8a-a3d1-4386-956c-dae749667676"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'What is blackbody radiation?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Blackbody radiation is a fundamental concept in physics, and I'm happy to explain it in simple terms.\\n\\nBlackbody radiation is the thermal radiation emitted by an idealized perfect absorber of electromagnetic radiation, known as a blackbody. In other words, it's the radiation emitted by an object that absorbs all the electromagnetic radiation that falls on it, rather than reflecting or transmitting any of it.\\n\\nWhen an object is heated, its atoms or molecules gain energy and start vibrating rapidly. As they vibrate, they emit electromagnetic radiation across a wide range of frequencies, including visible light, infrared, and even X-rays. The distribution of this radiation is characteristic of the object's temperature.\\n\\nThe key feature of blackbody radiation is that it follows a specific spectrum, known as Planck's law, which describes the energy distribution of the radiation as a function of frequency or wavelength. This spectrum is universal, meaning it's the same for all objects at a given temperature, regardless of their composition or shape.\\n\\nIn practice, blackbody radiation is important in many areas, such as:\\n\\n1. Thermal imaging: Blackbody radiation is used in thermal imaging cameras to detect temperature differences in objects.\\n2. Astronomy: The cosmic microwave background radiation is thought to be the residual heat from the Big Bang, and it's a form of blackbody radiation.\\n3. Materials science: Understanding blackbody radiation helps us design materials with specific thermal properties.\\n\\nI hope that helps! Do you have any follow-up questions on this topic?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"what is 2 + 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "JVuWLyeQlD8a",
        "outputId": "08ed9d94-a964-4cbd-ccd2-64d68db206fc"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'what is the value of 2 + 2'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"A classic!\\n\\nTo answer this question, I'll break it down into its simplest component parts:\\n\\n**Component Part 1:** 2\\n**Component Part 2:** 2\\n\\nNow, I'll answer each component part individually:\\n\\n**Component Part 1:** 2 = 2 (no calculation needed, it's just 2!)\\n\\n**Component Part 2:** 2 = 2 (again, no calculation needed, it's just 2!)\\n\\nNow, I'll combine the answers to the component parts to get the final answer:\\n\\n**Final Answer:** 2 + 2 = 2 + 2 = **4**\\n\\nSo, the value of 2 + 2 is **4**!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Default Chain Selection:**\n",
        "\n",
        "**Fallback Mechanism:** If the input does not match any of the criteria defined for the specific destination chains, the router chain will fall back to the `default_chain.`\n",
        "\n",
        "\n",
        "**Uncertainty Handling:** In some implementations, if the language model is unsure about the best route or if the confidence score for routing to a specific chain is below a certain threshold, it may also route the input to the `default_chain.`"
      ],
      "metadata": {
        "id": "58_UTfLvjXxI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"How old is lionel messi?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "7HK31DMwlFIO",
        "outputId": "22436f3f-1e38-4910-86be-72622dc22df9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "None: {'input': 'Who is Lionel Messi and how old is he?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"HELLO LEONARDO!\\n\\nLionel Messi is a professional soccer player who is widely regarded as one of the greatest players of all time. He was born on June 24, 1987, which makes him 35 years old as of June 2022. Messi has played for FC Barcelona and the Argentina national team, winning numerous accolades including six Ballon d'Or awards, ten La Liga titles, and four UEFA Champions League titles. He is known for his exceptional dribbling skills, speed, and goal-scoring ability.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    }
  ]
}