{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain: Models, Prompts and Output Parsers\n",
        "\n",
        "\n",
        "## Outline\n",
        "\n",
        " * Direct API calls to OpenAI\n",
        " * API calls through LangChain:\n",
        "   * Prompts\n",
        "   * Models\n",
        "   * Output parsers"
      ],
      "metadata": {
        "id": "5iMybGSLlUHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to install `langchain` and `langchain_groq`"
      ],
      "metadata": {
        "id": "OJMrIPGTlYZs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9594L36lh0w",
        "outputId": "5b1227eb-6d0b-40f4-da66-f21e05d3e333",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
            "Requirement already satisfied: langchain_groq in /usr/local/lib/python3.10/dist-packages (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.1)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.63)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain_groq) (0.8.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.11.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain langchain_groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Groq?**\n",
        "\n",
        "Groq, is an American AI company that builds an AI accelerator application-specific integrated circuit (ASIC) that they call the Language Processing Unit (LPU) and related hardware to accelerate the inference performance of AI workloads.\n",
        "\n",
        "Examples of the types AI workloads that run on Groq's LPU are:\n",
        "- large language models\n",
        "- image classification\n",
        "- anomaly detection\n",
        "- predictive analysis."
      ],
      "metadata": {
        "id": "bcVUxPCrlf7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models, Prompts and Parsers\n",
        "\n",
        "First, we need to get Groq API Key:"
      ],
      "metadata": {
        "id": "4HScjsXZvUJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"YOUR API KEY\""
      ],
      "metadata": {
        "id": "U2_ByMEWvXfh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `client.chat.completions.create()` is commonly used in the context of interacting with language model APIs, such as OpenAI's GPT-4, to generate completions based on a given prompt.\n",
        "\n",
        "### Explanation of Roles:\n",
        "- **System:** Provides context or instructions to the model.\n",
        "- **User:** Represents the input from the user.\n",
        "- **Assistant:** Represents the model's responses.\n",
        "\n",
        "### HyperParameters:\n",
        "### `top_p`\n",
        "- **Description:** `top_p` is a parameter for controlling the diversity of the generated text.\n",
        "\n",
        "- **Technical Details:** This parameter uses nucleus sampling (also known as probabilistic sampling). When generating text, instead of only considering the top tokens sorted by probability (`top_k`), the model considers the smallest possible set of tokens whose cumulative probability exceeds the value of `top_p`.\n",
        "- **Range:** The value of top_p ranges from $0$ to $1$.\n",
        "\n",
        "- **Effect:** A `top_p` value of $0.9$ means the model will consider the top $90$% of the probability mass. Lower values make the output more focused and deterministic, while higher values allow for more diversity and creativity in the output."
      ],
      "metadata": {
        "id": "7Fqs46f-aBY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `frequency_penalty`\n",
        "- **Description:** `frequency_penalty` is a parameter used to reduce the likelihood of the model repeating the same tokens within the generated output.\n",
        "\n",
        "- **Technical Details:** This penalty is applied to tokens based on their frequency in the generated text so far. Higher penalties make the model less likely to repeat the same word or phrase.\n",
        "\n",
        "- **Range:** The value typically ranges from 0 to 2, where 0 means no penalty, and higher values increase the penalty.\n",
        "\n",
        "- **Effect:** A higher `frequency_penalty` encourages the model to use a more diverse set of words. This can be useful for reducing repetitiveness and encouraging more varied responses."
      ],
      "metadata": {
        "id": "96_MeIeyd_I4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `presence_penalty`\n",
        "\n",
        "- **Description:** `presence_penalty` is a parameter that adjusts the likelihood of the model introducing new topics or ideas that haven't been mentioned yet in the conversation.\n",
        "\n",
        "- **Technical Details:** This penalty is applied to tokens that have not yet appeared in the generated text. Higher values encourage the model to explore new topics rather than sticking to those already discussed.\n",
        "\n",
        "- **Range:** The value typically ranges from 0 to 2, where 0 means no penalty, and higher values increase the penalty.\n",
        "\n",
        "- **Effect:** A higher `presence_penalty` encourages the model to be more creative and introduce new concepts into the conversation, potentially making the interaction more dynamic."
      ],
      "metadata": {
        "id": "nAqE6BF8eJFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "def get_completion(prompt):\n",
        "\n",
        "    client = Groq()\n",
        "    response = client.chat.completions.create(\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that knows a lot about celebrities\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        model = \"llama3-70b-8192\",\n",
        "        temperature = 0.01,\n",
        "        max_tokens = 512,\n",
        "        top_p = 0.5,\n",
        "        frequency_penalty = 0.5,\n",
        "        presence_penalty = 1\n",
        "    )\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "KHbiR-advXxO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = get_completion(\"Who is isabela moner?\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yoZG3O3vX4F",
        "outputId": "2d57cf78-79a3-4674-d1be-fa2dcff0e106"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-59316f58-a145-4954-b907-9260d2ff9755', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Isabela Moner is an American actress and singer. She was born on July 10, 2001, in Cleveland, Ohio. She is of Peruvian descent and is fluent in both English and Spanish.\\n\\nMoner began her acting career at a young age, making her screen debut in 2013 with a guest role on the Nickelodeon television series \"The Troop\". She then landed the lead role of CJ Martin on the Nickelodeon sitcom \"100 Things to Do Before High School\", which aired from 2014 to 2016.\\n\\nMoner\\'s breakthrough role came in 2017 when she played the lead role of Izabella in the film \"Transformers: The Last Knight\". She then starred as Dora in the 2019 live-action film \"Dora and the Lost City of Gold\", a reboot of the popular Nickelodeon animated series \"Dora the Explorer\".\\n\\nIn addition to her acting career, Moner is also a singer. She has released several singles and has collaborated with other artists on music projects.\\n\\nMoner is also known for her advocacy work, particularly in the area of environmental conservation. She has been involved with several charitable organizations and has used her platform to raise awareness about social and environmental issues.\\n\\nOverall, Isabela Moner is a talented and versatile young actress and singer who is making a name for herself in the entertainment industry!', role='assistant', function_call=None, tool_calls=None))], created=1716973977, model='llama3-70b-8192', object='chat.completion', system_fingerprint='fp_87cbfbbc4d', usage=CompletionUsage(completion_tokens=279, prompt_tokens=33, total_tokens=312, completion_time=0.832554485, prompt_time=0.018263837, queue_time=None, total_time=0.850818322), x_groq={'id': 'req_01hz1szdnme9atx9dnjem5xtsg'})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access to the output message content as following:"
      ],
      "metadata": {
        "id": "fbS_hZxeYUTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(output.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "NIuqomx1XK8E",
        "outputId": "d044f03a-1f2e-45a2-b797-ab62f93f580c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Isabela Moner is an American actress and singer. She was born on July 10, 2001, in Cleveland, Ohio. She is of Peruvian descent and is fluent in both English and Spanish.\n\nMoner began her acting career at a young age, making her screen debut in 2013 with a guest role on the Nickelodeon television series \"The Troop\". She then landed the lead role of CJ Martin on the Nickelodeon sitcom \"100 Things to Do Before High School\", which aired from 2014 to 2016.\n\nMoner's breakthrough role came in 2017 when she played the lead role of Izabella in the film \"Transformers: The Last Knight\". She then starred as Dora in the 2019 live-action film \"Dora and the Lost City of Gold\", a reboot of the popular Nickelodeon animated series \"Dora the Explorer\".\n\nIn addition to her acting career, Moner is also a singer. She has released several singles and has collaborated with other artists on music projects.\n\nMoner is also known for her advocacy work, particularly in the area of environmental conservation. She has been involved with several charitable organizations and has used her platform to raise awareness about social and environmental issues.\n\nOverall, Isabela Moner is a talented and versatile young actress and singer who is making a name for herself in the entertainment industry!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, Let's check some abilities of this model.\n",
        "\n",
        "for example, let's see how good the model is in transforming text into different contexts."
      ],
      "metadata": {
        "id": "HFi-6rf901ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Email:\n",
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse,\\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\"\n",
        "\n",
        "# Response Style\n",
        "style = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "rnwaYfaC041L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the corresponding prompt:\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks\n",
        "into a style that is {style}.\n",
        "text: ```{customer_email}```\n",
        "\"\"\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L71fP5KyvX5g",
        "outputId": "e63f60c1-f7ab-4ae7-af25-bdfcd9c1b2fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by triple backticks\n",
            "into a style that is American English in a calm and respectful tone\n",
            ".\n",
            "text: ```\n",
            "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we pass the `prompt` to the `get_completion()` function."
      ],
      "metadata": {
        "id": "cjYtmv6a1_D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(prompt)\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "l3QzRpto1jw_",
        "outputId": "69da089c-b11c-4d60-84b8-0c8e3717ce36"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here is the translated text in American English, in a calm and respectful tone:\n\n\"I'm extremely frustrated that my blender lid came loose and splattered my kitchen walls with smoothie. To make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, please.\"\n\nBy the way, have you heard about the time when actress Emma Stone had a kitchen mishap while making breakfast? She shared a funny story about it on The Ellen DeGeneres Show!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat API : LangChain\n",
        "\n",
        "Let's try how we can do the same using LangChain."
      ],
      "metadata": {
        "id": "XuK0tABf2REo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model:**"
      ],
      "metadata": {
        "id": "JAFEzcu82fUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "I9RXdGeEfxvX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can view the available models [here](https://console.groq.com/docs/models)\n",
        "\n",
        "This time, we use `mixtral-8x7b-32768` model.\n"
      ],
      "metadata": {
        "id": "1OYFS9Azgd3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_LLM = ChatGroq(\n",
        "    model = \"mixtral-8x7b-32768\",\n",
        "    temperature = 0.05\n",
        ")\n",
        "\n",
        "# We can access to the hyperparameters of the model:\n",
        "GROQ_LLM.temperature"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btI6W2Rb2nOn",
        "outputId": "7c9ead3b-f496-4670-d14f-56cd2c5dec5a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prompt template**\n",
        "The `ChatPromptTemplate.from_template()` function is typically used to create a chat prompt template in frameworks like LangChain, which are designed for building applications with language models. This function helps streamline the process of generating prompts by allowing you to define a template with placeholders that can be dynamically filled in during runtime.\n",
        "\n",
        "#### **Why use `ChatPromptTemplate`?**\n",
        "- **Reusability:** Define the template once and reuse it with different data.\n",
        "\n",
        "- **Maintainability:** Easier to manage and update prompt structures.\n",
        "\n",
        "- **Clarity:** Clear separation of the prompt structure and the dynamic content."
      ],
      "metadata": {
        "id": "Vw9OFC1c2hUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0zBhvKho2jJA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "prompt_template"
      ],
      "metadata": {
        "id": "_kjEb-TQ3cCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e96ba421-d97a-4ae9-88ca-47857150a41f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['style', 'text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n'))])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The prompt:\n",
        "print(prompt_template.messages[0].prompt, '\\n')\n",
        "\n",
        "# The prompt inputs:\n",
        "print(prompt_template.messages[0].prompt.input_variables, '\\n')\n",
        "\n",
        "# The prompt template:\n",
        "print(prompt_template.messages[0].prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XgnPO0V3zaC",
        "outputId": "85539433-8b34-4b68-ea61-17ea7efe6a40"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['style', 'text'] template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n' \n",
            "\n",
            "['style', 'text'] \n",
            "\n",
            "Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to set the `text` and `style` as inputs:  "
      ],
      "metadata": {
        "id": "vPBeJNqw40pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# style:\n",
        "customer_style = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\"\n",
        "\n",
        "# text:\n",
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cxVAoYaZ48bV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To feed the inputs to the prompt, we will use `.format_messages()`\n",
        "\n",
        "The purpose of `prompt_template.format_messages()` is to convert a prompt template into a sequence of messages that can be directly used in chat-based interactions with language models. This function helps in organizing the conversation context in a structured format."
      ],
      "metadata": {
        "id": "XajKxze1icyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_message = prompt_template.format_messages(\n",
        "    style = customer_style,\n",
        "    text = customer_email\n",
        ")"
      ],
      "metadata": {
        "id": "WTYcAqs03gPq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the customer_message\n",
        "print(customer_message)\n",
        "\n",
        "# Check its type\n",
        "print(type(customer_message))\n",
        "\n",
        "# Check the content\n",
        "print(customer_message[0])\n",
        "\n",
        "# Check the content type\n",
        "print(type(customer_message[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kUf7ZzC4PXd",
        "outputId": "eb35df5b-f72e-4e3d-ed93-fe1d5b772232"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\")]\n",
            "<class 'list'>\n",
            "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\"\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have constructed out prompt, we will feed it into our model:"
      ],
      "metadata": {
        "id": "EhYaIbU052de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_response = GROQ_LLM(\n",
        "    messages = customer_message\n",
        ")\n",
        "\n",
        "display(Markdown(customer_response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "WqtYh-Le57nN",
        "outputId": "8fc842e3-7023-458b-ce60-fb00752b720c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'm really upset that the lid of my blender flew off and splattered my kitchen walls with smoothie! To make matters worse, the warranty does not cover the cost of cleaning up my kitchen. I need your help right now, friend!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Response:**\n",
        "\n",
        "In the previous step, we styled the customer message. In the next step, we'll apply the same styling to the response that will be sent."
      ],
      "metadata": {
        "id": "0sLeRPrZ6C89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_reply = \"\"\"Hey there customer, \\\n",
        "the warranty does not cover \\\n",
        "cleaning expenses for your kitchen \\\n",
        "because it's your fault that \\\n",
        "you misused your blender \\\n",
        "by forgetting to put the lid on before \\\n",
        "starting the blender. \\\n",
        "Tough luck! See ya!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "service_style_pirate = \"\"\"\\\n",
        "a polite tone \\\n",
        "that speaks in English Pirate\\\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "S78TILiq6F-N"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we give the new styles to our pre-defined `prompt_template`, we mentioned that one of the advantages of defining a prompt template is the its reusability."
      ],
      "metadata": {
        "id": "kZYeF2n9j7x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_message = prompt_template.format_messages(\n",
        "    style = service_style_pirate,\n",
        "    text = service_reply\n",
        ")"
      ],
      "metadata": {
        "id": "TqYZZ0Fu6cXA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The final template:\n",
        "print(service_message[0].content, '\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBPvThYo8Vxx",
        "outputId": "416c538d-ff35-43d3-8a38-97f6d402743e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
            "```\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have created the `service_message`, which is prompt that want to rephrase the specifed `text` into an appropirate `style`."
      ],
      "metadata": {
        "id": "ZsfbMZfvkn1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "service_response = GROQ_LLM(service_message)\n",
        "\n",
        "display(Markdown(service_response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "c6iVkqUc68uO",
        "outputId": "9189ef44-b1e8-4b75-e74c-75c2ac1c282f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Ahoy there, me hearty customer! I'm afraid I've got some less-than-good news for ye. It appears that the warranty on yer kitchen gear doesn't cover the costs of cleanin' up after a mishap with yer blender. Ye see, it seems ye may have accidentally forgotten to put the lid on before startin' 'er up, and that, me friend, be considered misusin' the device.\n\nI can surely understand how these things happen, but, alas, the rules of the warranty be as they be. So, I'm afraid ye'll have to bear the costs of the cleanup on this one, mate. My apologies for the inconvenience, and may ye have better luck next time! Fare thee well!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Output Parsers**\n",
        "\n",
        "Sometimes, we want our model response be in a structure fomat, for that we use the `.with_structured_output()`."
      ],
      "metadata": {
        "id": "tP8WgMpYTN1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of `.with_structured_output()` is to enable the generation of outputs that adhere to a predefined structure, such as JSON objects, lists, or other data formats. This structured output is often necessary for integrating the language model's responses into larger systems or workflows where consistency and predictability of the output format are crucial."
      ],
      "metadata": {
        "id": "7taB_eXJloeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sODUh3KsTQF8",
        "outputId": "aadc2564-e10c-4a22-e201-71db6643b0be"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "\n",
        "text: {text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "E-xqtWKgT7-n"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to create our prompt template:"
      ],
      "metadata": {
        "id": "m0bl6c9qmRsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "prompt_template"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDgFkhx3T_In",
        "outputId": "f2385d41-7758-4394-a299-0b9719f5ebe0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'))])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `prompt_template` now takes a single input variable which is the `text`, so we need to feed it into our model."
      ],
      "metadata": {
        "id": "ATa5gEIRmkTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = prompt_template.format_messages(text = customer_review)\n",
        "message[0].content"
      ],
      "metadata": {
        "id": "zH4xW_hFUPwk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f7aa5885-dabe-4ef4-eba9-fc4140242c20"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, our input message is ready! we only need to give it to our model and also specify the `method`."
      ],
      "metadata": {
        "id": "2oBTibtHm71K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    model = \"llama3-70b-8192\",\n",
        "    temperature = 0.01\n",
        ")\n",
        "\n",
        "structured_llm = llm.with_structured_output(method = \"json_mode\")"
      ],
      "metadata": {
        "id": "FJCupb0L9XE0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = structured_llm.invoke(message)\n",
        "print(response)\n",
        "print(type(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1_t5DDQUrj2",
        "outputId": "09677ee9-ec44-4fba-942e-31255495147a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'gift': True, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['delivery_days']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guhB2Yz_kA18",
        "outputId": "960fcf1b-881b-4022-ab13-02a8c2b79651"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}